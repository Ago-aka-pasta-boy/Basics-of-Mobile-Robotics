{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06550659",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border:1px solid black; padding:20px 20px;text-align: justify;text-justify: inter-word\">\n",
    "    <strong>Project of basics of mobile robotics - Thymio in Paris<br/> Autumn 2021 <br/> </strong><br/>\n",
    "\n",
    "<div style=\"justify;text-justify: inter-word\">\n",
    "Bonvalot Isione <br>\n",
    "Franca Sébastien <br>\n",
    "In-Albon Malika <br>\n",
    "Pellegrini Sylvain <br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705c9f4",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction \n",
    "\n",
    "\n",
    "## Context\n",
    "During the course of Basics of Mobile Robotics, we were asked to complete a project combining five modules: Vision, Global Navigation, Local Navigation, Filtering and Motion Control. First we created an environment into which the robot can navigate. The objective is to cross under a bridge, go the the final goal and during that time avoid obstacles. The first part consists in getting the position of the elements in the setup: it is the vision part. Then the shortest path is computed thanks to the Global Navigation element. If an obstacle pops during the displacement of the robot, the Local Navigation will take care of avoiding it. The Filtering part predicts the position of the robot when the Vision part looses the robot's position. The Motion Control part computes the error between the robot's position and the path that he should follow. It controls the speed of the robot given the information of the different parts combined.\n",
    "\n",
    "<p align=\"justify\"> The story behind the project is the following. Thymio is on holiday in Paris. He had a long day, so he intends to stroll down the Champs Elysées and under the Arc of Triomphe by the shortest path through traffic. Then, Thymio wants to enjoy a view of the Eiffel Tower, still by the shortest path. During his walk, some Parisians in a hurry crosses his road but thanks to his good ability, Thymio avoids them and continues along the streets. </p>\n",
    "\n",
    "## Environment set-up\n",
    "<p> The ground where Thymio navigates is white to make the different elements of the setup appear more clearly. The components are listed below:</p>\n",
    "\n",
    "<ul>\n",
    "    <li> <b>Robot:</b> two blue circles on top, smaller circle defines the front of the robot\n",
    "    <li> <b>Goal:</b> 2D green circle</li>\n",
    "    <li> <b>Arc de Thriomphe:</b> 3D rectangle obstacle in red with brown sides. Thymio should pass under this obstacle</li>\n",
    "    <li> <b>Obstacles:</b> 2D triangles or squares are black</li>\n",
    "    <li> <b>Unforeseen obstacles:</b> 3D white cylinder</li>        \n",
    "</ul>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/report/original.jpg\" style=\"width:40%\">\n",
    "  <figcaption>Fig.1 - Top view of the setup</figcaption>\n",
    "</figure> \n",
    "\n",
    "The decision of making the obstacles in two dimensions has been taken to avoid conflict between local avoidance and global navigation. This way, Thymio can go very close to the permanent obstacles without detecting it and at the same time detect the unforeseen obstacles without trouble.\n",
    "\n",
    "<p> The camera used for the project is the camera provided in class. It is the AUKEY Overview Full HD Video 1080p Webcam. The camera is hung to a lamp on top of the set-up.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494dc6a9",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29046beb",
   "metadata": {},
   "source": [
    "## Get images and obstacles\n",
    "The external camera capture is initiated using the opencv library. Since the number assigned to an external camera changes with users, you should always check that the camera selected in the code is the right one. The CAP_DSHOW argument is very important as it drastically reduces the connection time to the camera (from 2 minutes down to less than a second).\n",
    "\n",
    "The vision first starts by extracting the obstacles from the red channel (3d dimension index 2, as the picture is in BGR format). It blurs it and then performs an opening. This filter consists of an erosion followed by a dilation. This reduces the noise in the image. The final pre-processing consists of a thresholding. The filtered image is then fed to the opencv findContours() function and the result is stored as an array of contours. They are then approximated to reduce computational cost and simply the obstacle assessment. Two characteristics are judged to classify a contour as an obstacle : a perimeter length neither over nor under what is expected, and the number of vertice must be contained between 3 and a maximum determined experimentally to take shadows into account.\n",
    "\n",
    "Next, using the extracted obstacles, it expands them by moving each vertex by a constant times the vector connecting it to the center of the obstacle. The center is found using the same equation using moments that will be presented in the next chapter. This is done to take Thymio’s width into account in the computations. Finally, the module also puts them in a slightly different format that fits the input requirements for the global pathing functions.\n",
    "\n",
    "The functions used to annotate a rough version of the environment are also included in the vision source file. They consist only of a combination of opencv drawing utilities that act on the canvas image provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd076b37",
   "metadata": {},
   "source": [
    "## Position of robot, arch and goal\n",
    "To detect the robot, arch and goal positions, a color and shape code has been defined. The robot is represented by two blue circles of different sizes, the red rectangle corresponds to the arch and the final goal is represented by a green circle. First of all, the function cv.inRange() of the opencv library produces some threshold images for each color. Then the different shapes are extracted to get the positions.\n",
    "\n",
    "\n",
    "### Find robot and goal position\n",
    "Detecting circles is achieved with the function cv.HoughCircles which requires a greyscale image as an input. Several conversions need to be done because the cv.inRange function needs a hsv image in input and delivers a threshold image as an output. As shown in Figure 2, the mask is then applied to the greyscale image. It is useful to blur the image before inserting it in the HoughCircles function, because it helps smoothing the contour of the masked image. The output is an array containing the circles center and radius.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/report/frame-grey_mask.png\" style=\"width:100%\">\n",
    "  <figcaption>Fig.2 - Mask applied to grayscale image</figcaption>\n",
    "</figure>\n",
    "\n",
    "To get the goal position, the function get_goal_position() returns the circle's center and radius. The green circle is detected with the detect_circles() function, which directly returns the center and the radius of the circle. The center is used to give the (x,y) coordinates of the goal position and the radius will be used in the function convert_meter2pxl() later in the project.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/report/frame-goal.png\" style=\"width:100%\">\n",
    "  <figcaption>Fig.3 - Different steps to get the goal position</figcaption>\n",
    "</figure>\n",
    "\n",
    "To return the robot position, the function get_robot_position() returns not only the (x, y) coordinates, but also an angle to give the direction of the thymio. First of all, the function detect_circles() is called to get the two circles center and radius. It has been defined that the smaller circle defines the front of the robot. The function calculates the angle between the two centers. The function finally returns the (x, y) coordinates of the larger circle and the angle between the two circles.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/report/frame-robot.png\" style=\"width:100%\">\n",
    "  <figcaption>Fig.4 - Different steps to detect the two circles of the robot</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Find \"Arc de Triomphe\" position\n",
    "The function get_arch_position() return two points (point1, point2) along which the robot should pass to go through the arch as shown in Figure 4. First of all, a threshold image on the red color is created with the cv.inRange() function, then the image is blurred.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/report/frame-arch.png\" style=\"width:100%\">\n",
    "  <figcaption>Fig.4 - Different steps to detect the arch and print point1 and point2</figcaption>\n",
    "</figure>\n",
    "\n",
    "The next step is finding the rectangle, it is done with the function cv.findContours() and cv.approxPolyDP() of the opencv library. To find point1 and point2, it is necessary to compute the center and the orientation of the arch. To do so, the image moment $M_{1,0}$, $M_{0,0}$, and $M_{0,1}$ are given with the function cv.moments of the opencv library. It is then possible to calculate the barycenter and the angle of the rectangle, the angle returned is between -90° and 90° counterclockwise.\n",
    "\n",
    "First the following formula gives the coordinates of the barycenter.\n",
    "\n",
    "<center> $cx = \\frac{M_{1,0}}{M_{0,0}}$ and $cy = \\frac{M_{0,1}}{M_{0,0}}$\n",
    "\n",
    "The angle is finally given by this formula:\n",
    "\n",
    "<center> $\\theta = -\\frac{1}{2}\\ tan^{-1}\\left( \\frac{2\\mu'_{1,1}}{\\mu'_{2,0}\\ \\mu'_{0,2}}\\right)$ with <br> <right> $\\mu'_{2,0} = \\frac{M_{2,0}}{M_{0,0}}-cx^2,\\quad \\mu'_{1,1} = \\frac{M_{1,1}}{M_{0,0}}-cx\\cdot cy,\\quad \\mu'_{0,2} = \\frac{M_{0,2}}{M_{0,0}}-cy^2$\n",
    "    \n",
    "Knowing the center of the arch and its orientation, it possible to find point1 and point2 with trivial trigonometry calculations.\n",
    "\n",
    " ### Sanity check \n",
    "All the function before return a boolean success value, additionaly to the value they should return. For instance if the functions detect more than one or two circles, if the arch is not detected as a retangle, etc.. It is useful to avoid problems because the vision does not always find the shapes due to inexactitudes. So in the while(True) there is no problem if sometimes the function return False, None. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb5c87",
   "metadata": {},
   "source": [
    "## Printing images of Thymio, Eiffel Tower and Arc de Triomphe\n",
    "To illustrate the project Thymio in Paris, the objective was to print images of Thymio, Arc de Triomphe and Eiffel Tower in the « mission status » image instead of drawing simple shapes like circles and arrows. However, a problem appeared because the time of execution of the loop took three times longer than usual (0.4 seconds instead of 0.15 seconds). It means that we had to adapt the refresh rate Ts of the Kalman filter which was too slow. The final decision was printing the fancy images only in the “environment” image during the state COMPUTE and keep the simple shapes drawing in the “mission status” image for the state TRAVEL.\n",
    "\n",
    "To print the image of the Arc de Triomphe and Eiffel Tower, there was no difficulty because only the x, y position had to be taken in account.  However, to print the robot not only the x, y position was important but also the angle of the robot. The first step is rotating the image of the Thymio with the functions cv.getRotationMatrix2D() and cv.warpAffine() of the opencv library. The second step is to adjust the position of the robot in the provided image to perfectly match the real robot position.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/report/environment.jpg\" style=\"width:40%\">\n",
    "  <figcaption>Fig.5 - Environment image with images of Thymio, Eiffel Tower and Arc of Triomphe printed</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90e754",
   "metadata": {},
   "source": [
    "---\n",
    "# Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7512adc",
   "metadata": {},
   "source": [
    "## Global navigation\n",
    "\n",
    "**For simplicity, the lists' indices used in code for the Global Navigation module follow this convention: <br>\n",
    "index 0 corresponds to start, maximal index corresponds to goal, and all intermediary indices correspond to obstacle vertices. <br>\n",
    "This allows us to work with points' indices (also referred to as names) rather than points' coordinates.**\n",
    "<br/>\n",
    "\n",
    "### Visibility graph\n",
    "After comparing the available options for creating a connected graph, we chose to use a visibility graph. In fact, a grid search does not appear as a good choice here because of the image distortion (due to the position of the camera), the difficulty of controlling the Thymio in a grid rather than in coordinates, the need for arbitrary parameters (e.g. size of the grid), and the heavyness of computation associated to the entire grid. In contrast, a visibility graph is quickly built once we know the obstacles' coordinates, and its implementation with the motion is straightforward due to the use of coordinates rather than a grid.\n",
    "\n",
    "\n",
    "\n",
    "The code for computing the visibility graph given all coordinates is given in global_path.py.<br/>\n",
    "The form of the output is a list of all points' neighbours' name and their distance to that point: <br>\n",
    "[[neighbours_of_start], [neighbours_of_vertex1],..., [neighbours_of_goal]] <br>\n",
    "It is used as the input for the shortest path algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5014dd2",
   "metadata": {},
   "source": [
    "#### Completing the list of neighbours\n",
    "\n",
    "<br/>\n",
    "\n",
    "We want to indicate for each point:\n",
    "- who its neighbours are\n",
    "- how far they are from that point<br>\n",
    "\n",
    "*(Note that the term \"neighbour\" refers to all points that are visible by the current point, i.e. no line connecting two neighbours passes trough an obstacle.)*\n",
    "<br/><br/>\n",
    "\n",
    "To simplify our search, we assume that our obstacles are simple shapes such as squares or triangles. Thanks to this assumption, we only have five different possibilities to determine whether two points p1,p2 are neighbours (their connection line **CL** is valid) or not: <br>\n",
    "\n",
    "1. (p1,p2) is a side of an obstacle $\\rightarrow$ CL stays valid (segment v1 on Figure 6)<br>\n",
    "2. p1 and p2 lie on the same obstacle, but CL is not a side of the obstacle $\\rightarrow$ CL becomes not valid (segment nv1 on the Figure 6)\n",
    "3. p1 and p2 do not lie on the same obstacle. p1 and/or p2 lie on all lines that CL intersects $\\rightarrow$ CL stays valid (segment v2 on Figure 6)\n",
    "4. p1 and p2 do not lie on the same obstacle. CL intersects some lines on which neither p1 nor p2 lie $\\rightarrow$ CL becomes not valid (segment nv2 on Figure 6)\n",
    "5. Other cases: we consider that CL stays valid (this did not cause any problems upon testing).\n",
    "\n",
    "Note that this logic is not robust to complicated obstacle shapes. On the following figure, nv3 would not be valid due to condition 2. However, this logic is sufficient with the kind of obstacles that we use.<br>\n",
    "<br/>\n",
    "\n",
    "\n",
    "#### Overlaps\n",
    "<br/>\n",
    "The obstacles used for the algorithm are enlarged versions of the real obstacles, to allow Thymio to pass in their neighbourhood without hitting them. This means that there is a risk of overlap between two obstacles.<br>\n",
    "To take it into account, we create a list with all obstacle vertices located within another obstacle (e.g. point 9 on Figure 6). If a point is within this list, we will bypass the above conditions and force it to have no neighbours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912ab08",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "  <img src=\"./images/VisibilityGraph.JPG\" alt=\"Trulli\" style=\"width:50%\">\n",
    "  <figcaption>Fig.6 - Result of the visibility graph code</figcaption>\n",
    "</figure\n",
    "    \n",
    "<br/>\n",
    "Note the indexing convention: point 0 is the start, point 15 is the goal, all intermediary indices are obstacles' vertices. <br/>\n",
    "\n",
    "Our algorithm considers that v1 and v2 are valid (conditions 1 and 3 respectively). <br>\n",
    "nv1,nv2,nv3 are not valid (conditions 2, 4, 2 respectively).\n",
    "\n",
    "The situation for nv4 is a bit more complex. It is a side of obstacle 12-13-14, hence considered valid at first (condition 1). But once we check nv4 with obstacle 9-10-11, it is considered invalid (condition 4). In conclusion, nv4 is considered not valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5dab9",
   "metadata": {},
   "source": [
    "For one point and one obstacle, the pseudo-code that implements this logic is as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "775f2013",
   "metadata": {},
   "source": [
    "choose a point p1\n",
    "    choose a point p2!=p1 that is not in list_overlaps\n",
    "        valid_connection = True                             #unless we prove it wrong\n",
    "        choose an obstacle\n",
    "            if cond1==True:\n",
    "                pass\n",
    "            elif cond2==True:\n",
    "                valid_connection = False\n",
    "                \n",
    "            else:\n",
    "                choose 2 vertices on the obstacle\n",
    "                if cond3==True:\n",
    "                    pass\n",
    "                elif cond4==True:\n",
    "                    valid_connection = False\n",
    "                else #cond5:\n",
    "                    pass\n",
    "         if valid_connection == True:                       #all tests ok\n",
    "             add (p2, dist_p1_p2) to p1's neighbours\n",
    "             \n",
    "    save p1's neighbours\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ed1f9",
   "metadata": {},
   "source": [
    "### Optimal path finding\n",
    "\n",
    "Now that the visibility graph is known, we can explore it to determine which path leads to the goal in an optimal way. This relies on the A* algorithm (the logic and pseudo-codes used in this section are inspired from the lecture slides and the Wikipedia page on A* algorithm: https://en.wikipedia.org/wiki/A*_search_algorithm, *cons. 08.12.2021*). <br/>\n",
    "\n",
    "The developed code can apply whether the Dijkstra (in case that the start and goal position are not specified) or the A* algorithm (used heuristic: Euclidean distance from the goal). For the final implementation we went with the A* algorithm, but both methods yielded similar results with no major difference in execution time. <br/>\n",
    "\n",
    "The code for finding the optimal path is given in shortest_path.py. <br/><br>\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "The initialization follows this order:<br>\n",
    "1. Calculate the visibility graph thanks to global_path.py\n",
    "2. Create a list with all points' names: [0, 1, ..., N, N+1] (following the convention 0$\\rightarrow$start and N+1$\\rightarrow$goal)\n",
    "3. Assign the heuristic to every point of the graph (0 if Dijkstra algorithm is used ; Euclidean distance to the goal if A* algorithm is used).\n",
    "4. Attribute infinite weight to every point of the graph, except the start (weight_start = 0 + heuristic)\n",
    "5. Initialize the following lists: \n",
    "    - predecessors, where we will store pieces of the optimal path before reconstructing it entirely. For example: \"To reach goal, come from vertex 5. To reach vertex 5, come from vertex 2. To reach vertex 2...\".\n",
    "    - vertices_to_explore, which will indicate what points on the graph are still unexplored. <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7063df8",
   "metadata": {},
   "source": [
    "#### Graph exploration <br/>\n",
    "\n",
    "The pseudo-code for graph exploration is given in the following cell. A few precisions:\n",
    "- current_neighbours contains the names of current_point's neighbours, but also their distance from current_point\n",
    "- predecessors is the list where pieces of the optimal path are stored for later reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ab62481",
   "metadata": {},
   "source": [
    "while there are still points to explore:\n",
    "    current_point = unexplored point with minimal weight\n",
    "    current_neighbours = list_neighbours[current_point] #name and distance\n",
    "    \n",
    "    for ngb in current_neighbours:\n",
    "        weight_temp = weight_current_point + ngb.dist + heuristic[ngb.name]\n",
    "        if weight_temp < weight_ngb:\n",
    "            weight_ngb = weight_temp\n",
    "            predecessors[ngb.name] = current_point\n",
    "            \n",
    "            if ngb.name was already explored:\n",
    "                add ngb.name to unexplored points #in case some weights need to be updated\n",
    "    \n",
    "    remove current_point from unexplored points\n",
    "    \n",
    "    if (A* is used) and (goal is a neighbour of current_point): #no need to explore the full graph\n",
    "        exit the while loop\n",
    "\n",
    "return predecessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e6e65",
   "metadata": {},
   "source": [
    "#### Reconstructing the path\n",
    "\n",
    "Once the list *predecessors* is filled, we work backwards to reconstruct the entire path until the start is reached. <br>\n",
    "For example, let's say predecessors = [\"reached_start\", 0, 0, 2, 1, 3]. The goal (index 5) is reached from index 3. Index 3 is reached from index 2. Index 2 is reached from index 0 (start). <br>\n",
    "The optimal path is then: [0, 2, 3, 5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef26e83",
   "metadata": {},
   "source": [
    "## Local navigation </br>\n",
    "\n",
    "<p> <i> The code was inspired by the exercises sessions 3 and 4 from the course MICRO-452 : Basics of mobile robotics. </i> </p>\n",
    "\n",
    "<p> The local navigation is used to avoid physical obstacles that are unpredictible. These obstacles can be put in Thymio's path at any point in time so that the Thymio should leave the optimal path to avoid them. Once the obstacle is passed, the robot has to go back to the initial optimal path. This is done by the motion control module. There are many possibilities to implement a local avoidance using the proximity sensors. </p> </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a204f",
   "metadata": {},
   "source": [
    "### Simplified ANN </br>\n",
    "\n",
    "#### Obstacle detection\n",
    "<p>To detect an obstacle, the five front proximity sensors are used. \n",
    "<br/>\n",
    "    \n",
    "<figure>\n",
    "  <img src=\"images/robot_sensors.png\" style=\"width:30%\">\n",
    "  <figcaption>Fig.7 - Caption - source : MICRO-452 moodle</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br/></p>\n",
    "<ul>\n",
    "    <li> To detect an obstacle on the left side : we calculate the mean of the first and second sensors.</li>\n",
    "    <li> To detect an obstacle on the right side : we calculate the mean of the fourth and fifth sensors. </li>\n",
    "    <li> To detect an obstacle in front of the robot : we measure the value of the third sensors.</li>\n",
    "</ul>\n",
    "</br>\n",
    "\n",
    "<p> The code to check if an obstacle is detected is given below. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBST_THR_L = 10      # low obstacle threshold \n",
    "OBST_THR_H = 60      # high obstacle threshold \n",
    "\n",
    "def check_obstacle(prox_sensors):\n",
    "    # acquisition from the proximity sensors to detect obstacles\n",
    "    obst = [prox_sensors[0], prox_sensors[1], \\\n",
    "            prox_sensors[2], prox_sensors[3], \\\n",
    "            prox_sensors[4]]\n",
    "    mean_obst_left = (obst[0] + obst[1])//2\n",
    "    mean_obst_right = (obst[3] + obst[4])//2\n",
    "    obst_front = obst[2]\n",
    "    \n",
    "    if (mean_obst_right > OBST_THR_H) or (mean_obst_left > OBST_THR_H) \\\n",
    "        or (obst_front > OBST_THR_H) :\n",
    "        obstacle = True\n",
    "    else:\n",
    "        obstacle = False\n",
    "    return obstacle, obst_front, mean_obst_left, mean_obst_right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a85db0",
   "metadata": {},
   "source": [
    "#### Pseudo-code\n",
    " <p> The following pseudo-code describes the different steps once an obstacle is detected somewhere in front of the robot.  </p>\n",
    "<p> There are two cases depending on the position of the unforseen obstacle. \n",
    "<ul>\n",
    "<li>Case 1 : Obstacle only in front of the robot (detected only by the third sensor) </li>\n",
    "    <ul>\n",
    "    <li> The robot turns in place to the left (tester de randomiser la direction qu'il prend ?</li>\n",
    "    <li> The speed is constant </li>\n",
    "    </ul>\n",
    "<li>Case 2 : Obstacle detected on either the left or the right side</li>\n",
    "    <ul>\n",
    "    <li> The robot accelerates in the opposite direction of the obstacle</li>\n",
    "    <li> The speed depends on the values of the sensors means </li>\n",
    "    </ul>\n",
    "</ul>  \n",
    "\n",
    "For the first case, to ensure that the robot moves straight forward to the obstacle, we check that the values of the wheels are almost the same. (tournure de phrase pas ouf je trouve)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545041e9",
   "metadata": {},
   "source": [
    "The code concerning the case 1 is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd56293",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (abs(motor_left_speed - motor_right_speed) < 50):\n",
    "    if obst_front > OBST_THR_H:\n",
    "        motor_left = -SPEED0\n",
    "        motor_right = SPEED0\n",
    "    else:\n",
    "        obst_in_front = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcd05b",
   "metadata": {},
   "source": [
    "The code concerning the case 2 is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53861c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if obst_in_front == 0:\n",
    "    motor_left = SPEED0 + OBST_SPEED_GAIN*(mean_obst_left//100)\n",
    "    motor_right = SPEED0 + OBST_SPEED_GAIN*(mean_obst_right//100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b1fd2",
   "metadata": {},
   "source": [
    "---\n",
    "# Motion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d9382",
   "metadata": {},
   "source": [
    "## Proportional controller \n",
    "<p> The robot's motion is controlled via a simple proportional controller. The idea is to calculate the angle error between the robot's position and its next goal's position. The next goals correspond to the different vertices given by the global path navigation. The error calculation is done by the function \"get_error(robot_pos, next_goal)\" in the motion_control.py file. </p>\n",
    "<p> Given the error, the wheels' speed need to be updated according to it. Compared to a classic proportional controller, ours is a little bit upgraded. In fact, we decided to divide the NORMAL_SPEED by the error in order to reach the good path in a smoother way; If the error is large, the robot will almost turn in place to reach the good path and if the error is small, the robot will correct its trajectory while going forward. This will allow the robot to have a smooth movement.\n",
    "This is done by the code given below in the function speed_control(err) of the motion_control.py file. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef24994",
   "metadata": {},
   "outputs": [],
   "source": [
    "    motor_left = NORMAL_SPEED/max(1,10*abs(err)) - KP*err\n",
    "    motor_right = NORMAL_SPEED/max(1,10*abs(err)) + KP*err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09493ea9",
   "metadata": {},
   "source": [
    "---\n",
    "# Kalman filtering\n",
    "## General structure\n",
    "\n",
    "A Kalman filtering is introduced in the project to enhance the position & angle tracking of the Thymio. We chose this kind of filtering because it relies on the assumption that its parameters are Gaussian random variables, which does not seem far from reality. Indeed, the vision will return the position of Thymio with some uncertainty but the majority of samples will fall around the same value. The same goes for values read from odometry. \n",
    "<br><br/>\n",
    "The logic implemented is very simple: knowing the state (position & angle) at step k-1 and the values returned from Thymio's odometry, predict the state at step k. If the vision is available, then also proceed to the update. The corresponding pseudo-code is given below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4d33df1",
   "metadata": {},
   "source": [
    "state, covariance = predict(state, covariance, odometry)\n",
    "\n",
    "if no_error_with_vision:\n",
    "    state, covariance = update(state, covariance, vision)\n",
    "    \n",
    "return state, covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ccd979",
   "metadata": {},
   "source": [
    "The general dynamic model used is the following:\n",
    "\n",
    "$\\mathbf{x}_{k+1} = A\\, \\mathbf{x}_{k} + T_s \\cdot \\mathbf{f}(\\theta_{k+1})\\cdot \\,[v_{\\text{right}}, v_{\\text{left}}]^T \\\\\n",
    "\\mathbf{y}_{k+1} = C \\, x_{k}$\n",
    "\n",
    "<br/> with:<br>\n",
    "- $\\mathbf{x}_k$ the state $[x,y,\\theta]^T$ at step $k$;\n",
    "- $\\mathbf{f}$ a non-linear function which comes from the differential drive model;\n",
    "- $T_s$ the sampling time in seconds;\n",
    "- $v_{\\text{right}}, v_{\\text{left}}$ the linear speed of wheels in pixels/s ; \n",
    "- $A = C = I_{3\\times3}$ equal to the identity matrix (we assume no dependance between the states). <br/> <br/>\n",
    "\n",
    "To get $v_{\\text{right}}$ and $v_{\\text{left}}$, the value read by odometry must be converted from its initial unit (which seems arbitrary) into m/s, then into pixels/s. The first conversion is non-linear, hence we rely on interpolation (with scipy.interpolate.interp1d) on measures taken in a 2.5m long corridor. The second conversion is assumed linear, so that we only need to multiply by a conversion factor (given by the vision upon initialization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bca7db",
   "metadata": {},
   "source": [
    "## Prediction step\n",
    "\n",
    "### From the non-linear field $f(\\theta_k)$ to an input matrix $B$\n",
    "\n",
    "As described above, we use the differential drive model:<br>\n",
    "$ \\begin{bmatrix} x \\\\ y \\\\ \\theta \\end{bmatrix}_{k+1} \n",
    "= \\frac{1}{2} \\begin{bmatrix} cos(\\theta_{k+0.5}) & cos(\\theta_{k+0.5})\\\\\n",
    "-sin(\\theta_{k+0.5}) & -sin(\\theta_{k+0.5})\\\\\n",
    "\\frac{1}{L} & -\\frac{1}{L}\\end{bmatrix}  \n",
    "\\cdot \\begin{bmatrix} v_{\\text{right}} \\\\ v_{\\text{left}}\\end{bmatrix}$ <br>\n",
    "with L the axle length of Thymio, and $\\theta_{k+0.5}$ a notation used for $\\frac{\\theta_k + \\theta_{k+1}}{2}$ (which should be more precise than using $\\theta_k$ or $\\theta_{k+1}$ only). <br><br/>\n",
    "\n",
    "The function $\\mathbf{f}$ represented by the 3-by-2 matrix is nonlinear and depends on the current state. However since $\\theta_k$ is known, it is possible to get an estimation of $\\mathbf{f}$ at each calling of the Kalman filter. To do so, we assume the following relation between $\\theta_{k}$ and $\\theta_{k+1}$: <br>\n",
    "$\\theta_{k+1} = \\theta_k + T_s \\cdot \\frac{v_{\\text{right}}-v_{\\text{left}}}{2L}$\n",
    "\n",
    "Leading to this new linearized but time-dependent model:\n",
    "$\\mathbf{x}_{k+1} = A\\, \\mathbf{x}_{k} + T_s \\cdot B_{k+1} \\cdot \\,[v_{\\text{right}}, v_{\\text{left}}]^T \\\\\n",
    "\\mathbf{y}_{k+1} = C \\, x_{k}$\n",
    "\n",
    "where $B_{k+1}$ is the value of $\\mathbf{f}$ calculated from $\\theta_{k+0.5}$.\n",
    "\n",
    "### Take into account the unperfect rolling of the wheels\n",
    "\n",
    "To predict Thymio's next position, we need to estimate the error due to unperfect rolling of the wheels. We assume that $v_{\\text{right}}, v_{\\text{left}}$ are independent and each follow this model: <br>\n",
    "$v = v_0 + \\delta \\; \\; \\;$  with $\\delta \\sim \\mathcal{N}(0, \\sigma_v)$ <br/>\n",
    "\n",
    "To transcribe this model into Python, we initialize a matrix Q where $\\delta_{\\text{right}}$ and $\\delta_{\\text{left}}$ are stored: <br>\n",
    "\n",
    "    Q = np.random.normal(0.0, STD_WHEELSPEED, (2,1))\n",
    "\n",
    "### Predict the next state, calculate the covariance\n",
    "\n",
    "Now that the matrix $B$ and the process noise are defined, we can give the new value of the state and the covariance matrix based on the lecture slides: <br>\n",
    "$x_{k+1} = A\\cdot x_k + T_s\\cdot B_{k+1}\\cdot [v_{\\text{right}}, v_{\\text{left}}]^T \\\\\n",
    "\\Sigma_{k+1} = (A^T \\cdot \\Sigma_k \\cdot A^T) + \\text{diag} (B\\cdot Q)$ <br>\n",
    "where diag$(B\\cdot Q)$ is used to go from the space of wheel uncertainty to the space of covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d813096",
   "metadata": {},
   "source": [
    "## Update step\n",
    "\n",
    "### Measure noise\n",
    "\n",
    "Once again, we assume that $x,y,\\theta$ are independant. The matrix that represents measure noise hence is diagonal. Its components follow the distributions $\\mathcal{N}(0,\\sigma_i^2)$ where $\\sigma_i$, $i = 1,2$ is the standard deviation introduced upon measuring coordinates ($i=1$) or $\\theta$ ($i=2$).<br>\n",
    "\n",
    "    R = np.diag([np.random.normal(0.0, STD_MEASURE_COORDS), \\\n",
    "                 np.random.normal(0.0, STD_MEASURE_COORDS), \\\n",
    "                 np.random.normal(0.0, STD_MEASURE_THETA)]) #measure noise\n",
    "\n",
    "Note that we trust the measures from camera more than the prediction made with our model. Hence, the standard deviations used for R should be lower or equal to the one used for Q.\n",
    "\n",
    "### Calculation\n",
    "\n",
    "This is done according to the lecture slides:<br>\n",
    "$i = y_{k+1} - Cx_{k+1}^{\\text{predicted}} \\\\\n",
    "S = (C \\cdot \\Sigma_{k+1}^{\\text{predicted}} \\cdot C^T) + R \\\\\n",
    "K = \\Sigma_{k+1}^{\\text{predicted}} \\cdot C^T \\cdot S^{-1} $ <br/>\n",
    "$x_{k+1}^{\\text{updated}} = x_{k+1}^{\\text{predicted}} + K\\cdot i\\\\\n",
    "\\Sigma_{k+1}^{\\text{updated}} = (I_{3\\times 3} - KC) \\Sigma_{k+1}^{\\text{predicted}}$<br>\n",
    "where $i$ is the innovation, $y$ is the state measured by the camera, and K is the optimal gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9c6bf",
   "metadata": {},
   "source": [
    "## Improving the robustness\n",
    "\n",
    "Upon testing, the implementation of the Kalman filter seemed fine overall, but it sometimes happened (for one frame) that the Thymio thought that its position was on the other side of the map, or that it just flipped 180°. This is why we introduced some conditions on the update step, depending on the robot's history.\n",
    "\n",
    "### Calibration\n",
    "\n",
    "To calibrate the Kalman filter, we made Thymio run into circles. Then we added a correction factor on angle and position on the model, and we calibrated these correction factors until the trajectory predicted by Thymio was close to its real trajectory. <br>\n",
    "After this calibration, our prediction model gave very satisfactory results.\n",
    "\n",
    "### Update only if the measure makes sense\n",
    "\n",
    "Sometimes, the vision encounters a problem for just one frame, estimating a wrong position and/or angle for the robot. Hence we implemented a simple logic: if the Thymio does not seem to have teleported then proceed to prediction and update. Otherwise, only proceed to prediction. <br><br/>\n",
    "Implementing this condition forces us to store the history of positions and angles. The average of the history *avg_history* is calculated, then the update is considered only if the Euclidean distance *d(avg_history.coords, update.coords)* and the absolute difference *abs(avg_history.theta - update.theta)* are within some thresholds. <br><br/>\n",
    "\n",
    "Implementing this logic considerably improved the smooth tracking of the position, by ignoring isolated wrong measures from the camera.\n",
    "\n",
    "### If the Thymio is lost, then trust the vision\n",
    "\n",
    "The problem of using these thresholds is that if the Kalman model gets too far from reality, then the values will always fall over the thresholds. To solve this issue, we implemented a second test: if the vision consistently returns the same position, we can take it as ground-truth in spite of the discrepancy with prediction. This is equivalent to bypassing the Kalman filter to force a \"hard-update\" with new values. <br/>\n",
    "\n",
    "This condition also requires to store the history of error between prediction and vision's measure, as well as the history of vision's measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c6c10",
   "metadata": {},
   "source": [
    "---\n",
    "# Finite state machine\n",
    "The main program is a loop that will go through the different states. First, it is initialised in the state COMPUTE where it assesses the environment captured by the camera and computes the optimal trajectory that the robot will have to follow. Then, when the path is calculated, the program enters the TRAVEL mode. This state will control the movement of the Thymio and, depending on the proximity sensor value, it can switch over to a local obstacle avoidance routine. When the robot reaches the last goal on his path, it enters the FINISH state and stops on the destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1352c5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "<p>To conclude, this project was a good way to learn how to maneuver group work.\n",
    "It includes five components : vision, motion control, global navigation, local navigation, and filtering. The interconnection of the modules was the main challenge of this project. It was interesting (and a little bit stressful sometimes) to see that the modules worked fine when they were independently tested but that they failed when put together. </p>\n",
    "<p> Another big hurdle to overcome during this project was the connection and the communication between the computer and the Thymio. Getting the values of the sensors and sending the instructions’ values was confusing considering the different ways to access the robot (ClientAsync, tmdclient, %%run_python, etc).</p>\n",
    "<p> This project allowed us to work with an external camera which was a great learning experience because we had to tackle limitations such as image deformations, shadows or stability of the camera.\n",
    "Finally, this project was fulfilling due to the multiple resources existing in python (librairies, methods,...) allowing us to build a complex program in a short span of time. </p>\n",
    "<p> Coordinating work on the python files was very easy thanks to the github that was set up (which link is provided in the resources). On the other hand, working together on a jupyter notebook proved to be quite an issue as git versioning does have a hard time with the html format. This situation led us to use team management skills to communicate and work effectively as a collective unit. </p>\n",
    "\n",
    "<p> All in all, the results obtained in this project are satisfactory but there are areas where it could still be improved upon. Indeed, some features were considered but not implemented due to the short deadline imposed. One stands out among them, which is handling the situations where a local avoidance leads the Thymio into a fixed obstacle. Even then, the implementation of theoretical models into a real-life project was definitely rewarding. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3f037",
   "metadata": {},
   "source": [
    "github link: https://github.com/Ago-aka-pasta-boy/Thymio-in-Paris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5542c",
   "metadata": {},
   "source": [
    "---\n",
    "# Main code to execute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387f0c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T19:39:07.907711Z",
     "start_time": "2021-12-12T19:39:05.327805Z"
    }
   },
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()\n",
    "await node.wait_for_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938c62a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T19:39:09.288265Z",
     "start_time": "2021-12-12T19:39:08.936949Z"
    }
   },
   "outputs": [],
   "source": [
    "import vision as vis\n",
    "import kalman\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2 as cv\n",
    "import shortest_path as short\n",
    "import positions as pos\n",
    "import math\n",
    "import motion_control as motion\n",
    "import drawing as draw\n",
    "\n",
    "COMPUTE = 0\n",
    "TRAVEL = 1\n",
    "LOCAL_NAV = 2\n",
    "FINISH = 3\n",
    "\n",
    "SPEED0 = 50       # nominal speed\n",
    "OBST_SPEED_GAIN = 10  # /100 (actual gain: 5/100=0.05)\n",
    "SENSOR_SCALE = 200  # Scale factors for sensors\n",
    "MAX_SLEEP_TIME = 1\n",
    "SLEEP_STEP = 0.2\n",
    "\n",
    "OBST_THR_L = 10      # low obstacle threshold \n",
    "OBST_THR_H = 60      # high obstacle threshold \n",
    "\n",
    "state = 0          # 0= go to goal, 1=obstacle avoidance\n",
    "obst = [0,0,0,0,0]   # measurements from the 2 left and the 2 right prox sensors\n",
    "\n",
    "#Values for kalman filter\n",
    "HISTORY_SIZE = 10 #to ignore discrete mismatch predict/update\n",
    "ERR_HISTORY_SIZE = 5 #to hard-reset in case of continuous mismatch predict/update\n",
    "CAM_HISTORY_SIZE = 5 #number of samples used to take the mean, in case of hard-reset\n",
    "\n",
    "statego2goal = 0\n",
    "stateobstavoid = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131ee19",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-12T19:39:21.096Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main -> state machine\n",
    "state = COMPUTE\n",
    "obstacle = False\n",
    "has_entered_local = 0\n",
    "\n",
    "#valeurs pour initialiser\n",
    "robot_pos_kalman = np.array([[0],[0],[0]]) #3-by-1 np.array\n",
    "Sigma_kalman = np.diag([0.01,0.01,0.01])\n",
    "history_kalman = [np.reshape(robot_pos_kalman,(1,3))]*HISTORY_SIZE\n",
    "\n",
    "# Read images of robot, eiffel tower and arch of triomphe\n",
    "img_thymio = cv.imread('images/code/thymio.png', cv.IMREAD_UNCHANGED)\n",
    "img_eiffel = cv.imread('images/code/eiffel_tower.png', cv.IMREAD_UNCHANGED)\n",
    "img_arch = cv.imread('images/code/arch.png', cv.IMREAD_UNCHANGED)\n",
    "\n",
    "cap = cv.VideoCapture(2, cv.CAP_DSHOW)  \n",
    "cap.set(cv.CAP_PROP_AUTO_EXPOSURE, 1)\n",
    "while True:\n",
    "    if state == COMPUTE:\n",
    "        captured, img = cap.read()\n",
    "        if captured:\n",
    "            # cv.imshow(\"Original\", img)        # uncomment to debug\n",
    "            cv.imshow(\"Original\", img)\n",
    "            im3 = np.zeros(img.shape, np.uint8)\n",
    "            found, obstacles = vis.extract_obstacles(img)\n",
    "            if found:\n",
    "                ex_obstacles = vis.expand_obstacles(obstacles)\n",
    "                cv.drawContours(im3, ex_obstacles, -1, (0, 255, 0), 3)\n",
    "                cv.imshow(\"expanded\", im3)\n",
    "\n",
    "            found_rob, robot_pos = pos.get_robot_position(img)\n",
    "            found_goal, goal_pos, radius_pxl = pos.get_goal_position(img)\n",
    "            found_arch, arch_pos = pos.get_arch_positions(img)\n",
    "\n",
    "            if found_arch and found_rob:\n",
    "                if math.dist(arch_pos[0],robot_pos[0])>math.dist(arch_pos[1],robot_pos[0]):\n",
    "                    arch_pos = np.flip(arch_pos,0)\n",
    "\n",
    "            if (found and found_rob and found_goal and found_arch):\n",
    "                scale_factor = pos.convert_meter2pxl(radius_pxl)\n",
    "                vis.annotate_arch(arch_pos, im3)\n",
    "\n",
    "                AXLE_LENGTH = 0.095*scale_factor #used for kalman filter\n",
    "\n",
    "                vertices = vis.convert_vertice(ex_obstacles)\n",
    "                pathname = short.find_shortest_path(vertices, tuple(robot_pos[0]), tuple(arch_pos[0]))\n",
    "                path_arch =\\\n",
    "                short.pathname_to_coords(pathname,vertices,tuple(robot_pos[0]), tuple(arch_pos[0]))\n",
    "                vis.annotate_path(path_arch,im3)\n",
    "\n",
    "                pathname2 = short.find_shortest_path(vertices, tuple(arch_pos[1]), tuple(goal_pos))\n",
    "                path_goal =\\\n",
    "                short.pathname_to_coords(pathname2,vertices,tuple(arch_pos[1]), tuple(goal_pos))\n",
    "                vis.annotate_path(path_goal,im3)\n",
    "                # print images of robot, eiffel tower and arc de triomphe\n",
    "                draw.annotate_robot(robot_pos, im3, img_thymio, scale_factor)\n",
    "                draw.annotate_eiffel_tower(goal_pos, im3, img_eiffel, scale_factor)\n",
    "                draw.annotate_arch(arch_pos, im3, img_arch, scale_factor)\n",
    "                cv.imshow(\"environment\", im3)\n",
    "\n",
    "                path = []\n",
    "                path.extend(path_arch)\n",
    "                path.extend(path_goal)\n",
    "\n",
    "                #initialize kalman filter\n",
    "                robot_pos_kalman = np.array([[robot_pos[0][0]],[robot_pos[0][1]],[robot_pos[1]]]) #3-by-1 np.array\n",
    "                Sigma_kalman = np.diag([0.01,0.01,0.01])\n",
    "                history_kalman = [np.reshape(robot_pos_kalman,(1,3))]*HISTORY_SIZE\n",
    "                camera_history = [np.reshape(robot_pos_kalman,(1,3))]*CAM_HISTORY_SIZE\n",
    "                errpos_history = [0]*ERR_HISTORY_SIZE\n",
    "                errtheta_history = [0]*ERR_HISTORY_SIZE\n",
    "                Ts = 0.15\n",
    "\n",
    "                state = TRAVEL\n",
    "        else:\n",
    "            print(\"There was a problem in the capture\")\n",
    "            break\n",
    "    if state == TRAVEL:\n",
    "        cv.imshow(\"environment\", im3)\n",
    "        cv.waitKey(0)\n",
    "        \n",
    "        for subgoal in path:\n",
    "            while not motion.check_robot_arrived(robot_pos,subgoal):\n",
    "                prox_sensors = np.array([x for x in node['prox.horizontal'][0:5]])\n",
    "                print(prox_sensors)\n",
    "                obstacle, obst_front, mean_obst_left, mean_obst_right = motion.check_obstacle(prox_sensors)\n",
    "                \n",
    "                if obstacle == True:\n",
    "                    has_entered_local = MAX_SLEEP_TIME\n",
    "                    start_time = time.time()\n",
    "                    # obstacle avoidance: accelerate wheel near obstacle\n",
    "                    captured, img_local = cap.read()\n",
    "                    im4 = np.zeros(img_local.shape,np.uint8)\n",
    "                    vis.annotate_robot(robot_pos, im4)\n",
    "                    cv.drawContours(im4, obstacles, -1, (0, 255, 0), 3)    \n",
    "                    vis.annotate_path(path,im4)\n",
    "                    cv.putText(im4, \"Local avoidance\", (40, 40), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    cv.imshow(\"mission status\", im4)\n",
    "                    cv.imshow(\"Feed\",img_travel)\n",
    "                    cv.waitKey(10)\n",
    "                    obst_in_front = 1\n",
    "                    \n",
    "                    await node.wait_for_variables({\"motor.left.speed\"})\n",
    "                    await node.wait_for_variables({\"motor.right.speed\"})\n",
    "                    motor_left_speed = node.v.motor.left.speed\n",
    "                    motor_right_speed = node.v.motor.right.speed\n",
    "                    if (abs(motor_left_speed - motor_right_speed) < 50):\n",
    "                        if obst_front > OBST_THR_H:\n",
    "                            motor_left = -SPEED0\n",
    "                            motor_right = SPEED0\n",
    "                        else:\n",
    "                            obst_in_front = 0\n",
    "\n",
    "                    if obst_in_front == 0:\n",
    "                        motor_left = SPEED0 + OBST_SPEED_GAIN*(mean_obst_left//100)\n",
    "                        motor_right = SPEED0 + OBST_SPEED_GAIN*(mean_obst_right//100)\n",
    "                    \n",
    "                    #Kalman\n",
    "                    #read values from camera & sensors\n",
    "                    success_camera, robot_pos_camera = pos.get_robot_position(img_local)\n",
    "                    #print(\"robot angle\", robot_pos_camera[1])\n",
    "                    if success_camera:\n",
    "                        camera_input = [(robot_pos_camera[0][0],robot_pos_camera[0][1]), robot_pos_camera[1], success_camera]\n",
    "                        print(\"seen by camera\", camera_input)\n",
    "                    else:\n",
    "                        camera_input = [(0,0),0,False]\n",
    "                    await node.wait_for_variables({\"motor.left.speed\"})\n",
    "                    await node.wait_for_variables({\"motor.right.speed\"})\n",
    "                    motorspeed = [node.v.motor.right.speed, node.v.motor.left.speed]        \n",
    "\n",
    "\n",
    "                    #get position from kalman filter\n",
    "                    robot_pos_kalman, Sigma_kalman, history_kalman, errpos_history, errtheta_history, camera_history =\\\n",
    "                    kalman.kalmanfilter(robot_pos_kalman,Sigma_kalman,motorspeed,\\\n",
    "                                        history_kalman, camera_input, Ts, scale_factor, AXLE_LENGTH,\\\n",
    "                                       errpos_history, errtheta_history, camera_history)           \n",
    "                    robot_pos = [(int(robot_pos_kalman[0][0]), int(robot_pos_kalman[1][0])), robot_pos_kalman[2][0]]\n",
    "\n",
    "                    Ts = time.time() - start_time\n",
    "                else: #if obstacle == False                    \n",
    "                    start_time = time.time()      \n",
    "                    \n",
    "                    captured, img_travel = cap.read()\n",
    "                    cv.imshow(\"Feed\",img_travel)\n",
    "                    im4 = np.zeros(img_travel.shape)\n",
    "                    cv.drawContours(im4, obstacles, -1, (0, 255, 0), 3) \n",
    "                    vis.annotate_path(path,im4)\n",
    "                    vis.annotate_robot(robot_pos,im4)\n",
    "\n",
    "                    #read values from camera & sensors\n",
    "                    success_camera, robot_pos_camera = pos.get_robot_position(img_travel)\n",
    "                    #print(\"robot angle\", robot_pos_camera[1])\n",
    "                    if success_camera:\n",
    "                        camera_input = [(robot_pos_camera[0][0],robot_pos_camera[0][1]), robot_pos_camera[1], success_camera]\n",
    "                        print(\"seen by camera\", camera_input)\n",
    "                    else:\n",
    "                        camera_input = [(0,0),0,False]\n",
    "                    await node.wait_for_variables({\"motor.left.speed\"})\n",
    "                    await node.wait_for_variables({\"motor.right.speed\"})\n",
    "                    motorspeed = [node.v.motor.right.speed, node.v.motor.left.speed]        \n",
    "\n",
    "\n",
    "                    #get position from kalman filter\n",
    "                    robot_pos_kalman, Sigma_kalman, history_kalman, errpos_history, errtheta_history, camera_history =\\\n",
    "                    kalman.kalmanfilter(robot_pos_kalman,Sigma_kalman,motorspeed,\\\n",
    "                                        history_kalman, camera_input, Ts, scale_factor, AXLE_LENGTH,\\\n",
    "                                       errpos_history, errtheta_history, camera_history)           \n",
    "                    robot_pos = [(int(robot_pos_kalman[0][0]), int(robot_pos_kalman[1][0])), robot_pos_kalman[2][0]]\n",
    "                    #et apres, on passe robot_pos au controleur\n",
    "\n",
    "\n",
    "                    err = motion.get_error(robot_pos,subgoal)\n",
    "                    cv.putText(im4, \"Global\", (40, 40), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv.imshow(\"mission status\", im4)\n",
    "                    cv.waitKey(10)\n",
    "                    if has_entered_local <= 0.01:\n",
    "                        motor_left, motor_right = motion.speed_control(err)\n",
    "                        motor_left = round(motor_left)\n",
    "                        motor_right = round(motor_right)\n",
    "                    else:\n",
    "                        motor_left = 200\n",
    "                        motor_right = 200\n",
    "                        #set it instantly\n",
    "                        v = {\n",
    "                            \"motor.left.target\": [motor_left],\n",
    "                            \"motor.right.target\": [motor_right],\n",
    "                        }\n",
    "                        aw(node.set_variables(v))                        \n",
    "                        time.sleep(SLEEP_STEP)\n",
    "                        has_entered_local -= SLEEP_STEP \n",
    "                        \n",
    "                v = {\n",
    "                    \"motor.left.target\": [motor_left],\n",
    "                    \"motor.right.target\": [motor_right],\n",
    "                }\n",
    "                aw(node.set_variables(v))\n",
    "                \n",
    "                Ts = time.time() - start_time\n",
    "        state = FINISH\n",
    "    if state == FINISH:\n",
    "        v = {\n",
    "                    \"motor.left.target\": [0],\n",
    "                    \"motor.right.target\": [0],\n",
    "                }\n",
    "        aw(node.set_variables(v))\n",
    "            #print(\"motor_left\",motor_left)\n",
    "            #print(\"motor_right\", motor_right)\n",
    "            #motor_left_target = int(motor_left)\n",
    "            #motor_right_target = int(motor_right)\n",
    "            #print(\"motor_left_t\",motor_left_target)\n",
    "            #print(\"motor_right_t\", motor_right_target)          \n",
    "    cv.imshow(\"environment\",im3)\n",
    "    cv.waitKey(10)\n",
    "cap.release()\n",
    "print(\"finished!\")\n",
    "# cv.destroyAllWindows()\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5bb98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T10:45:20.578959Z",
     "start_time": "2021-12-12T10:45:20.464891Z"
    }
   },
   "outputs": [],
   "source": [
    "await node.unlock()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
